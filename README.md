# Time Series Forecasting: RNN vs. LSTM Comparative Analysis

This project explores time series forecasting using **Recurrent Neural Networks (RNN)** and **Long Short-Term Memory (LSTM)** architectures. The goal was to predict future values of a complex, noisy time series signal generated by superimposing three cosine functions, specifically comparing the models' ability to handle long-term dependencies across various input sequence lengths.

---

## üìä Project Overview

### Data Generation
[cite_start]The synthetic time series data ($\sim 5000$ steps) was created by combining three cosine functions (A, B, and C) [cite: 9-12][cite_start], normalizing the result, and adding Gaussian noise ($\mu=0, \sigma=0.2$)[cite: 25, 27, 28].

* [cite_start]**Total Data Points:** 5001 [cite: 9, 27]
* [cite_start]**Training Data:** First 3000 time steps [cite: 34]
* [cite_start]**Test Data:** Remaining 2001 time steps [cite: 34]



[Image of Time Series Data]


### ‚öôÔ∏è Model Implementation

Both the RNN and LSTM models were implemented in **PyTorch** to perform step-ahead prediction, using the output of the previous prediction as the input for the next (autoregressive forecasting).

* **Model Architectures:** Both models used 1 input feature, 64 hidden units, and 1 layer.
* **Training Objective:** Minimize Mean Squared Error (MSE) loss.
* [cite_start]**Experimentation:** Models were trained using four different **sequence lengths** (look-back windows): **3, 30, 100, and 300**[cite: 38].

---

## üìà Key Results and Analysis

### 1. Training and Test Performance

Models were evaluated by comparing the Mean Squared Error (MSE) on both the training data and the known test data.



**Finding 1: LSTM is Superior for Longer Dependencies**
* The **LSTM model consistently achieved lower Test MSE** across sequences of length 30, 100, and 300.
* The most significant advantage was seen at $100$ steps (RNN MSE: $0.0486$, LSTM MSE: $0.0429$), confirming the LSTM's effectiveness in learning and generalizing long-term patterns compared to the basic RNN.



### 2. Long-Horizon Autoregressive Prediction

The best models (Sequence Length 300) were then used to forecast **5000 steps** into the future by feeding the model's prediction back as the input for the next step.




**Finding 2: Both Models Struggle with Extended Horizons**
* Both models quickly **collapsed to a constant value** shortly after the prediction started at Time Step 3000.
* The high **Future Prediction MSE** (RNN: $1.423$, LSTM: $1.502$) confirms that neither architecture maintained the complex, multi-frequency oscillation pattern over thousands of steps when relying purely on its own potentially flawed outputs. This highlights the inherent difficulty of **autoregressive forecasting** over very long horizons.

---

### üíª Setup and Usage

The entire analysis is contained within the provided Jupyter Notebook.

1.  **Clone the Repository:**
    ```bash
    git clone [https://github.com/your-username/time-series-forecasting-rnn-lstm.git](https://github.com/your-username/time-series-forecasting-rnn-lstm.git)
    ```
2.  **Install Dependencies:** Ensure you have Python, PyTorch, NumPy, and Matplotlib installed.
3.  **Run the Notebook:** Open and run `h24wikar_ML_Lab5.ipynb` in a Jupyter environment (or Google Colab) to regenerate the data, train the models, and view all results and plots.

---

## üõ†Ô∏è Files

* `h24wikar_ML_Lab5.ipynb`: Main code and analysis notebook.
* `/images`: Directory containing project plots.
* `Assignment.docx`: The original assignment prompt (for full project context).
